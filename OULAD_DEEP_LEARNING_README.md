# OULAD Deep Learning Implementation

This implementation adds comprehensive modern deep learning approaches to the GUIDE pipeline for the OULAD (Open University Learning Analytics) dataset.

## Overview

The implementation includes:

### ðŸ—ï¸ Modern Deep Learning Architectures
- **TabNet**: Google's neural network specifically designed for tabular data
- **FT-Transformer**: Feature Tokenizer + Transformer architecture
- **NODE**: Neural Oblivious Decision Trees
- **SAINT**: Self-Attention and Intersample Attention Transformer
- **AutoInt**: Automatic Feature Interaction Learning

### ðŸ”§ Advanced Training Techniques
- **Mixup & CutMix**: Data augmentation for tabular data
- **Self-supervised pre-training**: Masked feature reconstruction
- **Contrastive learning**: Feature representation learning
- **Knowledge distillation**: Teacher-student model framework
- **Label smoothing**: Regularization technique
- **Progressive training**: Growing network complexity

### ðŸŽ¯ Hyperparameter Optimization
- **Bayesian optimization** with Optuna
- **Multi-objective optimization** (accuracy vs fairness)
- **Progressive training strategies**
- **Cross-validation** with proper OULAD splits

### ðŸ“Š Comprehensive Evaluation Framework
- **Performance benchmarking** across all models
- **Statistical significance testing**
- **Fairness evaluation** with demographic parity and equalized odds
- **Visualization and reporting**
- **Model comparison** with efficiency analysis

## Files Structure

```
src/oulad/
â”œâ”€â”€ modern_deep_learning.py              # Core modern architectures
â”œâ”€â”€ hyperparameter_optimization.py       # Optuna-based optimization
â”œâ”€â”€ advanced_training_techniques.py      # Advanced training methods
â”œâ”€â”€ comprehensive_evaluation.py          # Evaluation framework
â”œâ”€â”€ run_comprehensive_deep_learning.py   # Main integration script
â”œâ”€â”€ oulad_deep_learning_cli.py           # CLI integration
â””â”€â”€ ...

simple_oulad_test.py                     # Simple test script
```

## Quick Start

### 1. Basic Test

Run a simple test to verify the implementation:

```bash
cd /home/runner/work/GUIDE/GUIDE
python simple_oulad_test.py
```

### 2. CLI Interface

Use the integrated CLI:

```bash
# Basic mode - quick test
python src/oulad/oulad_deep_learning_cli.py --mode basic

# Comprehensive mode (placeholder for full implementation)
python src/oulad/oulad_deep_learning_cli.py --mode comprehensive --trials 100
```

### 3. Integration with Main GUIDE CLI

The OULAD deep learning functionality can be integrated into the main GUIDE CLI by adding the commands from `oulad_deep_learning_cli.py`.

## Architecture Details

### TabNet Implementation
- Attention-based feature selection
- Sequential decision steps
- Interpretable feature masks
- Ghost batch normalization

### FT-Transformer Implementation
- Feature tokenization for tabular data
- Multi-head self-attention
- Position embeddings for features
- Layer normalization and residual connections

### NODE Implementation
- Oblivious decision trees
- Differentiable tree traversal
- Ensemble of multiple trees
- Feature binning with soft decisions

### SAINT Implementation
- Row-wise attention mechanism
- Intersample attention for batch interactions
- Contrastive pre-training support
- Feature embeddings with positional encoding

### AutoInt Implementation
- Automatic feature interaction learning
- Multi-head attention for feature crossing
- Residual connections
- End-to-end differentiable

## Advanced Features

### 1. Hyperparameter Optimization

```python
from hyperparameter_optimization import OptunaTuner

tuner = OptunaTuner(
    model_type='tabnet',
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val,
    n_trials=100
)

study = tuner.optimize()
best_params = study.best_params
```

### 2. Multi-objective Optimization

```python
from hyperparameter_optimization import MultiObjectiveTuner

tuner = MultiObjectiveTuner(
    model_type='ft_transformer',
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val,
    sensitive_features=sensitive_features,
    n_trials=50
)

study = tuner.optimize()
pareto_solutions = study.best_trials
```

### 3. Advanced Training Techniques

```python
from advanced_training_techniques import train_with_advanced_techniques

results = train_with_advanced_techniques(
    model=model,
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val,
    use_pretraining=True,
    use_mixup=True,
    epochs=100
)
```

### 4. Comprehensive Evaluation

```python
from comprehensive_evaluation import run_comprehensive_evaluation

results = run_comprehensive_evaluation(
    X_train=X_train,
    y_train=y_train,
    X_test=X_test,
    y_test=y_test,
    sensitive_features=sensitive_features,
    optimize_hyperparameters=True,
    n_optimization_trials=50
)
```

## Performance Characteristics

### Model Complexity
- **SimpleTabularNN**: ~10K parameters
- **TabNet**: ~50-200K parameters (configurable)
- **FT-Transformer**: ~100-500K parameters
- **NODE**: ~20-100K parameters
- **SAINT**: ~50-300K parameters
- **AutoInt**: ~30-150K parameters

### Training Time (estimated on CPU)
- **SimpleTabularNN**: ~30 seconds
- **TabNet**: ~2-5 minutes
- **FT-Transformer**: ~3-7 minutes
- **NODE**: ~1-3 minutes
- **SAINT**: ~3-8 minutes
- **AutoInt**: ~2-4 minutes

### Expected Performance on OULAD
Based on literature and similar datasets:
- **Baseline (Simple NN)**: ~60-65% accuracy
- **TabNet**: ~68-73% accuracy
- **FT-Transformer**: ~70-75% accuracy
- **NODE**: ~65-70% accuracy
- **SAINT**: ~70-76% accuracy
- **AutoInt**: ~67-72% accuracy

## Current Status

### âœ… Completed
- Core architecture implementations
- Hyperparameter optimization framework
- Advanced training techniques
- Evaluation and comparison framework
- CLI integration
- Simple test validation

### âš ï¸ Known Issues
- TabNet dimension handling needs refinement for some configurations
- Full comprehensive evaluation pipeline needs integration testing
- Visualization components need matplotlib backend configuration

### ðŸ”„ Future Enhancements
- GPU optimization and distributed training
- AutoML pipeline integration
- Real-time inference API
- Integration with fairness-aware ML frameworks
- Deployment containerization

## Usage Examples

### Example 1: Train Single Model

```python
from modern_deep_learning import TabNet, train_model
from sklearn.model_selection import train_test_split

# Load and split data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)

# Create model
model = TabNet(input_dim=X.shape[1], n_d=64, n_a=64, n_steps=5)

# Train
results = train_model(
    model=model,
    train_loader=train_loader,
    test_loader=val_loader,
    epochs=100,
    lr=0.01,
    weight_decay=1e-4,
    device='cuda'
)
```

### Example 2: Compare Multiple Models

```python
from modern_deep_learning import train_modern_deep_learning_models

results = train_modern_deep_learning_models(
    X_train=X_train,
    y_train=y_train,
    X_test=X_test,
    y_test=y_test
)

# Results contain all model performances
for model_name, metrics in results['results'].items():
    print(f"{model_name}: AUC = {metrics['test_auc']:.4f}")
```

### Example 3: Hyperparameter Optimization

```python
from hyperparameter_optimization import run_comprehensive_optimization

optimization_results = run_comprehensive_optimization(
    X_train=X_train,
    y_train=y_train,
    X_val=X_val,
    y_val=y_val,
    n_trials=100
)

# Get best parameters for each model
for model_type, results in optimization_results.items():
    print(f"Best {model_type} params: {results['best_params']}")
```

## Dependencies

Core dependencies:
- torch >= 1.12.0
- scikit-learn >= 1.0.0
- pandas >= 1.3.0
- numpy >= 1.20.0
- optuna >= 3.0.0
- matplotlib >= 3.5.0
- seaborn >= 0.11.0

Optional dependencies:
- cuda toolkit (for GPU acceleration)
- tensorboard (for monitoring)

## Testing

Run the test suite:

```bash
# Simple functionality test
python simple_oulad_test.py

# CLI test
python src/oulad/oulad_deep_learning_cli.py --mode basic

# Full test with modern architectures (when fixed)
python src/oulad/run_comprehensive_deep_learning.py --mode basic
```

## Contributing

When contributing to this implementation:

1. Follow the existing code structure and naming conventions
2. Add comprehensive docstrings to new functions and classes
3. Include type hints for function parameters and returns
4. Test new functionality with the simple test script
5. Update this documentation for new features

## References

1. TabNet: Attentive Interpretable Tabular Learning (Arik et al., 2021)
2. Revisiting Deep Learning Models for Tabular Data (Gorishniy et al., 2021)
3. Neural Oblivious Decision Trees for Deep Learning on Tabular Data (Popov et al., 2019)
4. SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training (Somepalli et al., 2021)
5. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks (Song et al., 2019)

## License

This implementation follows the same license as the main GUIDE repository.